# Goal 3: "Support the Development and Dissemination of Advanced Data Management, Analytics, and Visualization Tools"



##  The appropriateness of the goals of the plan and of the strategies and implementation tactics proposed to achieve them

Goal 3: The strategy to leverage and support existing tool-sharing systems to
encourage "marketplaces" for tools developers, and to separate funding of tool
development and data generation is very important, and we support this direction
of the NIH. This is a proven strategy to elevate high quality tools, for
example, in the world of high-throughput genomics, one can consider the NHGRI
funded Bioconductor Project as a decade-long successful use case in providing a
unified interface for more than 1,000 "high-quality, open-source data
management, analytics, and visualization tools".

Goal 3: In general, implementation tactics are plausible, but not evidence-based.
Rather than propose that each step NIH takes to develop a tactic is supported by
a body of research, the lowest-hanging fruit (and most productive solution) is
to have the community develop an actual set of strategic targets, with clear
metrics for evaluation.

Goal 3: The SPDS plan underestimates the pervasiveness and persistence of
bad/outdated software and methods (See: https://www.the-scientist.com/?articles.view/articleNo/51260/title/Scientists-Continue-to-Use-Outdated-Methods/). It is completely unclear how separating evaluation and
funding for tool development and dissemination from support for databases and knowledgebases (this sentence from the SPDS is itself unclear) will address
this problem. This may help, but is our knowledge an unvetted hypothesis.

Goal 3: Although the SPDS does not make any strategy clear, the goal of
supporting tools and workflows (objective 3-1) is a good one. We further agree
that partnership is exactly the way that this needs to be pursued.

Goal 3: The metrics for this sophisticated set of objectives are catastrophic.
There is no way that the objectives stated for Goal 3 can be effectively
measured or succeed.

## Opportunities for NIH to partner in achieving these goals

Goal 3: There are a variety of groups the NIH can partner with. The number of
potential individual investigators is too numerous to list, but these
individuals should be relatively easy to identify by means of their scholarly
contributions (carefully avoiding journal publications as a primary metric).
Reaching out and partnering with groups such as the Open Bioinformatics
Foundation and societies like ISMB would be an ideal way for NIH to foster deep
community involvement. 

##  Additional concepts that should be included in the plan

Goal 3: The proposal as currently mentioned does not mention (1) computational reproducibility, or (2) exploratory data analysis for data quality control.
These two topics are critical for the high-level goal of "extracting
understanding from large-scale or complex biomedical research data".

Goal 3: Computational reproducibility can be defined as the ability to produce
identical results from identical data input or "raw data", and relies on
biomedical researchers keeping track of metadata regarding the versions of
tools that were used, the way in which tools were run, and the provenance and
version of publicly available annotation files if these were used. This is very
important for data science: if two groups observe discrepancies between their
results, they absolutely must be able to identify the source, whether it be
methodological or due to different versions of software or annotation data.

Goal 3: Exploratory data analysis (EDA) needs to be a key component of the data
science plan, as this should be the first step of any data analysis involving
complex biological data. EDA is often how a data scientist will identify data
artifacts, technical biases, batch effects, outliers, unaccounted for or
unexpected heterogeneity, need for data transformation, or other various data
quality issues that will cause serious problems for downstream methods, whether
they be statistical methods, machine learning, deep learning, artificial
intelligence or otherwise. Downstream methods may either fail to detect the
relevant signal (loosely categorized as "false negatives") or may produce many
spurious results which are purely associations with technical aspects of the
data ("false positives"). Furthermore, Basic EDA can uncover biological signal
that may be missed, such as biologically relevant heterogeneity, e.g. subtypes
of disease with signal present in molecular data.

Goal 3: Computational reproducibilty and supporting EDA should be components of
both NIH funded tool development, as well as the plan to "Enhance Workforce
Development for Biomedical Data Science" (Goal 4).

## Performance measures and milestones that could be used to gauge the success of elements of the plan and inform course corrections



## Any other topic the respondent feels is relevant for NIH to consider in developing this strategic plan
