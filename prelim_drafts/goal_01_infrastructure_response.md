# Goal 1: "Support a Highly Efficient and Effective Biomedical Research Data Infrastructure"



##  The appropriateness of the goals of the plan and of the strategies and implementation tactics proposed to achieve them

Goal 1: NIH should be applauded for recognizing that historically, funding of
data resources used funding approaches that were research projects. We agree
this must change. Also, we agree that tool development and data resources may
often need distinct funding contexts/expectations.

Goal 1: We agree with the stated need to support the hardening and optimization
(user-friendliness, computational efficiency, etc.) of innovative tools and
algorithms.

Goal 1: The Data Commons has generally been driven from bottom-up development by
the community. The Commons is in its early stages and should be allowed to
function as is. The Commons is the appropriate testbed for many of the
technological innovations and processes that NIH may ultimately which to explore
at broader scales after sufficient development.

Goal 1: The implementation tactics proposed seem almost randomly selected,
making it nearly impractical to criticize them. For example, one of the three
bullet points indicates that new technologies should be adapted (somewhat
meaningless, but not disagreeable), but then goes on to mention that GPUs should
be used. That is weirdly specific, and not necessarily wrong, but why mention at
this level of detail without some specific vision for the real biomedical
informatics problems that are relevant here? This is like saying “calculus”
should be used. Maybe; but such an out of context statement is ultimately a
collection of buzzwords. At best, this is an indication that greater expertise
is needed to reformulate this document.

Goal 1: Although this is a strategy document, it’s bewildering to imagine the
linking of NIH datasets as described in objective 1-2 can be elucidated in a
single thin paragraph. I have no idea how the “Biomedical Data Translator” will
work but can only imagine it will need to function at least as well as the
“Universal Translator” of Star Trek. Either enough detail for the strategy needs
to be proposed here for the document to serve its purpose, or the space is
better spent articulating the hard problems that need fixing.

## Opportunities for NIH to partner in achieving these goals

Goal 1: NSF has been exploring centralized computing models through XSEDE, and
open-science clouds (CyVerse Atmosphere, XSEDE-Jetstream) for many years. These
groups would be natural partners in addition to commercial cloud providers. The
NSF resources will not match the capacity of commercial cloud but have optimized
for the science use-cases and user profiles relevant to biomedical research.

##  Additional concepts that should be included in the plan

Goal 1: Grafting “Data Science” onto NIH is essentially a massive retrofitting
exercise. If I had to pick one area to think of, it is how emerging techniques
(Long-read sequencing, machine learning approaches, CIRSPR, etc.) and how we
treat that related data should be a primary target before the essentially get
too big to go back. The community of users is smaller, and fixing emerging
challenges seems like a more manageable focus for fomenting community consensus. 


## Performance measures and milestones that could be used to gauge the success of elements of the plan and inform course corrections

Goal 1: The proposed evaluation metrics are horrific. These metrics are more
appropriate for cloud providers who capture the described metrics to develop
their invoices. While NIH should look to drive down communal costs, Goal 1,
like all of the goals in this document are – hard research problems –  which
require deep thought to understand what success is.

## Any other topic the respondent feels is relevant for NIH to consider in developing this strategic plan

Goal 1: The SPDS NIH correctly identifies that “The generation of most
biomedical data is highly distributed and is accomplished mainly by individual
scientists or relatively small groups of researchers.” This should be followed
by the conclusion that any top-down approach must be matched by a
correspondingly large-scale bottom-up approach. Individual investigators need
the training and support to generate data in a way that fulfills the promise of
FAIR principles. The Strategic Plan quotes the famous (and regrettable)
statistic that 80% of Data Science is cleaning data, yet nothing proposed in
this document will solve this. While NIH is in a position to pioneer (and
appropriately fund) hard infrastructure (computation/storage, etc.), the greater
attention must be paid to funding soft-infrastructure – training, documentation,
and support that bring investigators into a community of practice. Note that
Barone et.al (https://doi.org/10.1371/journal.pcbi.1005755 ) replicates the
earlier findings of EDUCAUSE (https://net.educause.edu/ir/library/pdf/ers0605/rs/ers0605w.pdf); organizations planning for cyberinfrastructure development tend to underestimate
and underfund the training needed to use infrastructure. Any infrastructure
development must be matched by clear, measurable learning outcomes to ensure
that investigators can actually make intended use of the investments.
